from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Tuple
import os
import json
from textwrap import dedent

from dotenv import load_dotenv
from openai import OpenAI

# ğŸ”¹ RAG ê´€ë ¨ import
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# .envì—ì„œ OPENAI_API_KEY ì½ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

client = OpenAI(api_key=OPENAI_API_KEY)
embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)


# FAISS ë²¡í„° DB ì¤€ë¹„


def build_faiss_from_json(folder_path: str = "./embedding",
                          index_path: str = "faiss_index") -> None:
    """
    DevBê°€ ë§Œë“  JSON íŒŒì¼ë“¤(question_templates, answer_patterns, ë“±)ì„ ì½ì–´ì„œ
    FAISS ë²¡í„° DBë¥¼ í•œ ë²ˆ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    docs: List[Document] = []

    filenames = [
        "question_templates.json",
        "answer_patterns.json",
        "competency_rubrics.json",
        "model_answers.json",
    ]

    for filename in filenames:
        filepath = os.path.join(folder_path, filename)
        if not os.path.exists(filepath):
            print(f" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filepath}")
            continue

        with open(filepath, "r", encoding="utf-8") as f:
            items = json.load(f)
            # ê° itemì€ {"page_content": "...", "metadata": {...}} í˜•íƒœë¼ê³  ê°€ì •
            for item in items:
                docs.append(
                    Document(
                        page_content=item["page_content"],
                        metadata=item.get("metadata", {}),
                    )
                )

    if not docs:
        raise ValueError("ì„ë² ë”©ì— ì‚¬ìš©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    vectorstore = FAISS.from_documents(docs, embedding)
    vectorstore.save_local(index_path)
    print("FAISS ë²¡í„° DB ìƒì„± ì™„ë£Œ")


def load_vectorstore(index_path: str = "faiss_index") -> FAISS:
    if not os.path.exists(index_path):
        build_faiss_from_json()
    db = FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)
    print("FAISS ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
    return db


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ (ì „ì—­)
vectorstore = load_vectorstore()


async def get_rag_context(
    job: str,
    company: str,
    question: str,
    answer: str,
    k: int = 4,
) -> Tuple[str, List[str]]:
    """
    ì§ˆë¬¸/ë‹µë³€/ì§ë¬´/íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISSì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    - GPTì— ë„£ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    - ê° ë¬¸ì„œì˜ 'source' ê°™ì€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    ë¥¼ ë°˜í™˜.
    """
    query = f"{question}\n\n{answer}\n\nì§ë¬´: {job}, íšŒì‚¬: {company}"

    docs = vectorstore.similarity_search(query, k=k)

    context_texts: List[str] = []
    sources: List[str] = []

    for doc in docs:
        context_texts.append(doc.page_content)

        meta = doc.metadata or {}
        # DevB JSONì— ë“¤ì–´ ìˆëŠ” í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ì¶œì²˜ë¡œ ì‚¬ìš©
        source = (
            meta.get("source")
            or meta.get("question_id")
            or meta.get("pattern_id")
            or meta.get("competency_id")
            or meta.get("id")
            or ""
        )
        sources.append(str(source))

    context = "\n\n---\n\n".join(context_texts)
    return context, sources



# 1. ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜


class FeedbackRequest(BaseModel):
    job: str
    company: str
    question: str
    answer: str

class FeedbackResponse(BaseModel):
    summary: str
    logic: str
    concreteness: str
    fit: str
    delivery: str
    next_tips: List[str]
    example_answer: str
    retrieved_sources: List[str]



# 2. í”„ë¡¬í”„íŠ¸ ì •ì˜


SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¸ì‚¬ë‹´ë‹¹ìì´ì ë©´ì ‘ ì½”ì¹˜ì…ë‹ˆë‹¤.

ì§€ì›ìì˜ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
- ë…¼ë¦¬ì„±: ë§ì˜ íë¦„, êµ¬ì¡°, í•µì‹¬ ë©”ì‹œì§€ê°€ ëª…í™•í•œì§€
- êµ¬ì²´ì„±: ê²½í—˜, ìˆ˜ì¹˜, ê¸°ê°„, ì—­í•  ë“± êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆëŠ”ì§€
- ì§ë¬´ ì í•©ì„±: ë‹µë³€ì´ ì§€ì› ì§ë¬´ì™€ ì˜ ì—°ê²°ë˜ëŠ”ì§€
- ì „ë‹¬ë ¥: ë©´ì ‘ ìƒí™©(1~2ë¶„ ë‹µë³€)ì— ë§ê²Œ í•µì‹¬ì´ ì˜ ì „ë‹¬ë˜ëŠ”ì§€

ì§€ì›ìê°€ ë°”ë¡œ ë‹¤ìŒ ì—°ìŠµì— í™œìš©í•  ìˆ˜ ìˆë„ë¡
'ë¬´ì—‡ì„ ì–´ë–»ê²Œ ê³ ì³ì•¼ í•˜ëŠ”ì§€'ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.
ë§‰ì—°í•œ ì¹­ì°¬ë³´ë‹¤ í–‰ë™ ì§€ì¹¨ì— ê°€ê¹Œìš´ í”¼ë“œë°±ì„ ì„ í˜¸í•©ë‹ˆë‹¤.
"""

def build_user_prompt(
    job: str,
    company: str,
    question: str,
    answer: str,
    context_docs: str = ""
) -> str:
    job_desc = job or "íŠ¹ì • ì§ë¬´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë©´ì ‘ ì—­ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”."
    company_desc = company or "íŠ¹ì • íšŒì‚¬ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    context_part = context_docs if context_docs else "ë³„ë„ì˜ ì°¸ê³  ìë£ŒëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    return dedent(f"""
    [ì§ë¬´ ì •ë³´]
    {job_desc}

    [íšŒì‚¬ ì •ë³´]
    {company_desc}

    [ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ì°¸ê³  ìë£Œ)]
    {context_part}

    [ë©´ì ‘ ì§ˆë¬¸]
    {question}

    [ì§€ì›ìì˜ ë‹µë³€]
    {answer}

    [ìš”ì²­ì‚¬í•­]
    1. ìœ„ ë‹µë³€ì— ëŒ€í•´ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.
       - ì–´ë–¤ ì ì´ ì¢‹ê³  ì–´ë–¤ ì ì´ ë¶€ì¡±í•œì§€ ì„œìˆ 
       - í•„ìš”í•œ ê²½ìš° ê°œì„  ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”

    2. í•´ë‹¹ ë‹µë³€ì„ STAR êµ¬ì¡°ë¡œ ë¶„ì„í•˜ì—¬ ê° ìš”ì†Œë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì½”ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
       - ì˜ˆì‹œ:
         - Situation: 3/5 - ë°°ê²½ ì„¤ëª…ì´ ëª¨í˜¸í•¨
         - Task: 4/5 - ëª©í‘œê°€ ëª…í™•í•˜ê²Œ í‘œí˜„ë¨
         - Action: 2/5 - êµ¬ì²´ì ì¸ í–‰ë™ ì„¤ëª…ì´ ë¶€ì¡±í•¨
         - Result: 3/5 - ì„±ê³¼ê°€ ëšœë ·í•˜ì§€ ì•ŠìŒ

    3. ê°œì„ ëœ ë‹µë³€(ëª¨ë²”ë‹µë³€)ì„ STAR êµ¬ì¡°ì— ë§ì¶° ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
       - ê° ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    ```json
    {{
      "summary": "ì „ë°˜ í‰ê°€ 2~3ë¬¸ì¥",
      "logic": "ë…¼ë¦¬ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "concreteness": "êµ¬ì²´ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "fit": "ì§ë¬´ ì í•©ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "delivery": "ì „ë‹¬ë ¥ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "next_tips": [
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  1",
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  2",
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  3"
      ],
      "example_answer": "ê°œì„ ëœ ì˜ˆì‹œ ë‹µë³€(8~12ë¬¸ì¥)",
      "retrieved_sources": []
    }}
    ```

    ì„¤ëª… ë¬¸ì¥ì€ ì“°ì§€ ë§ê³ , ìœ„ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    """)



# 3. GPT í˜¸ì¶œ í•¨ìˆ˜


def call_gpt(job: str, company: str, question: str, answer: str, context_docs: str = "") -> str:
    user_prompt = build_user_prompt(job, company, question, answer, context_docs)

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",  # í•„ìš”ì— ë”°ë¼ ëª¨ë¸ ë³€ê²½ ê°€ëŠ¥
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.4,
    )

    content = resp.choices[0].message.content
    return content



# 4. JSON íŒŒì„œ


def parse_feedback(raw: str) -> FeedbackResponse:
    cleaned = raw.strip()

    # ```json ... ``` ê°ì‹¸ì§„ ê²½ìš° ì œê±°
    if cleaned.startswith("```"):
        cleaned = cleaned.strip("`")
        if cleaned.startswith("json"):
            cleaned = cleaned[4:]
        cleaned = cleaned.strip()

    try:
        data = json.loads(cleaned)
    except Exception:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: raw ì „ì²´ë¥¼ summaryì— ë„£ê³  ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ê°’
        return FeedbackResponse(
            summary=cleaned[:500],
            logic="",
            concreteness="",
            fit="",
            delivery="",
            next_tips=[],
            example_answer="",
            retrieved_sources=[],
        )

    summary = data.get("summary", "")
    logic = data.get("logic", "")
    concreteness = data.get("concreteness", "")
    fit = data.get("fit", "")
    delivery = data.get("delivery", "")
    next_tips = data.get("next_tips", [])
    example_answer = data.get("example_answer", "")
    retrieved_sources = data.get("retrieved_sources", [])

    # íƒ€ì… ë°©ì–´: next_tips / retrieved_sources ê°€ ë¬¸ìì—´ë¡œ ì˜¬ ìˆ˜ë„ ìˆìŒ
    if isinstance(next_tips, str):
        next_tips = [next_tips]
    if not isinstance(next_tips, list):
        next_tips = []

    if isinstance(retrieved_sources, str):
        retrieved_sources = [retrieved_sources]
    if not isinstance(retrieved_sources, list):
        retrieved_sources = []

    return FeedbackResponse(
        summary=summary,
        logic=logic,
        concreteness=concreteness,
        fit=fit,
        delivery=delivery,
        next_tips=next_tips,
        example_answer=example_answer,
        retrieved_sources=retrieved_sources,
    )



# 5. FastAPI ì—”ë“œí¬ì¸íŠ¸ /ë‹µë³€ í…ŒìŠ¤íŠ¸ìš© fast api


app = FastAPI()

@app.post("/ai-feedback", response_model=FeedbackResponse)
async def ai_feedback(request: FeedbackRequest):
    # 1) RAGë¡œ ì»¨í…ìŠ¤íŠ¸ + ì¶œì²˜ ê°€ì ¸ì˜¤ê¸°
    try:
        context_docs, sources = await get_rag_context(
            request.job,
            request.company,
            request.question,
            request.answer,
        )
    except Exception as e:
        print(" RAG ì—ëŸ¬, ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰:", e)
        context_docs = ""
        sources = []

    # 2) GPT í˜¸ì¶œ
    raw = call_gpt(
        job=request.job,
        company=request.company,
        question=request.question,
        answer=request.answer,
        context_docs=context_docs,
    )

    # 3) JSON íŒŒì‹±
    feedback = parse_feedback(raw)

    # 4) ì¶œì²˜ ì •ë³´ ë®ì–´ì“°ê¸°
    feedback.retrieved_sources = sources

    return feedback
