# ìƒˆë¡œìš´ ê²€ìƒ‰ ê·œì¹™ ê¸°ë°˜ API ì—”ë“œí¬ì¸íŠ¸
from fastapi import Body
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Tuple
import os
import json
from textwrap import dedent

from dotenv import load_dotenv
from openai import OpenAI

# ğŸ”¹ RAG ê´€ë ¨ import
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# .envì—ì„œ OPENAI_API_KEY ì½ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

client = OpenAI(api_key=OPENAI_API_KEY)
embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY)


# FAISS ë²¡í„° DB ì¤€ë¹„


def build_faiss_from_json(folder_path: str = "src/embedding",
                          index_path: str = "faiss_index") -> None:
    """
    DevBê°€ ë§Œë“  JSON íŒŒì¼ë“¤(question_templates, answer_patterns, ë“±)ì„ ì½ì–´ì„œ
    FAISS ë²¡í„° DBë¥¼ í•œ ë²ˆ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    docs: List[Document] = []

    filenames = [
        "question_templates.json",
        "answer_patterns.json",
        "competency_rubrics.json",
        "answer_templates.json",
        "company_values.json",
    ]

    for filename in filenames:
        filepath = os.path.join(folder_path, filename)
        if not os.path.exists(filepath):
            print(f" íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filepath}")
            continue

        with open(filepath, "r", encoding="utf-8") as f:
            items = json.load(f)
            # ê° itemì€ {"page_content": "...", "metadata": {...}} í˜•íƒœë¼ê³  ê°€ì •
            for item in items:
                docs.append(
                    Document(
                        page_content=item["page_content"],
                        metadata=item.get("metadata", {}),
                    )
                )

    if not docs:
        raise ValueError("ì„ë² ë”©ì— ì‚¬ìš©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. JSON íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    vectorstore = FAISS.from_documents(docs, embedding)
    vectorstore.save_local(index_path)
    print("FAISS ë²¡í„° DB ìƒì„± ì™„ë£Œ")


def load_vectorstore(index_path: str = "faiss_index") -> FAISS:
    if not os.path.exists(index_path):
        build_faiss_from_json()
    db = FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)
    print("FAISS ë²¡í„° DB ë¡œë“œ ì™„ë£Œ")
    return db


# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ (ì „ì—­)
vectorstore = load_vectorstore()


async def get_rag_context(
    job_group: str,
    job: str,
    company: str,
    question: str,
    answer: str,
    k: int = 4,
) -> Tuple[str, List[str]]:
    """
    ì§ˆë¬¸/ë‹µë³€/ì§ë¬´/íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAISSì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    - GPTì— ë„£ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
    - ê° ë¬¸ì„œì˜ 'source' ê°™ì€ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    ë¥¼ ë°˜í™˜.
    """
    query = dedent(f"""
{question}
{answer}

ì§êµ°: {job_group}
ì§ë¬´: {job}
íšŒì‚¬: {company}

# íšŒì‚¬ ì¸ì¬ìƒ ìš°ì„  ê²€ìƒ‰ì„ ìœ„í•œ ì˜ë„ì  ë¶€ìŠ¤íŒ… í‚¤ì›Œë“œ
[{company} ì¸ì¬ìƒ í•µì‹¬ê°€ì¹˜ íšŒì‚¬ê°€ ì›í•˜ëŠ” ì¸ì¬ìƒ í•µì‹¬ì—­ëŸ‰ ê¸°ì—…ë¬¸í™”]

""")

    docs = vectorstore.similarity_search(query, k=k)
    

    context_texts: List[str] = []
    sources: List[str] = []

    for doc in docs:
        context_texts.append(doc.page_content)

        meta = doc.metadata or {}
        # DevB JSONì— ë“¤ì–´ ìˆëŠ” í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ì¶œì²˜ë¡œ ì‚¬ìš©
        source = (
            meta.get("source")
            or meta.get("question_id")
            or meta.get("pattern_id")
            or meta.get("competency_id")
            or meta.get("id")
            or ""
        )
        sources.append(str(source))

    context = "\n\n---\n\n".join(context_texts)
    return context, sources



# 1. ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜



# ê¸°ì¡´ í”¼ë“œë°± ìš”ì²­ ëª¨ë¸ (ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ í˜¸í™˜)

class FeedbackRequest(BaseModel):
    job_group: str
    job: str
    company: str
    question: str
    answer: str
    standard_question: str = "n"  # "y" or "n"
    question_id: str = None        # standard_question=="y"ì¼ ë•Œë§Œ ì‚¬ìš©

# ìƒˆë¡œìš´ ì§ˆë¬¸ ëœë¤ ì¶”ì¶œ ìš”ì²­ ëª¨ë¸
class RandomQuestionRequest(BaseModel):
    q_list: list[str]  # ì˜ˆ: ["tech", "tenacity", ...]
    count: int         # ê° íƒ€ì…ë³„ ì¶”ì¶œ ê°œìˆ˜

# ìƒˆë¡œìš´ ì§ˆë¬¸ ëœë¤ ì¶”ì¶œ ì‘ë‹µ ëª¨ë¸
class RandomQuestionGroup(BaseModel):
    question_type: str
    questions: list[dict]  # ê° ì§ˆë¬¸: {"page_content": ..., "metadata": {...}}

class RandomQuestionResponse(BaseModel):
    groups: list[RandomQuestionGroup]

class FeedbackResponse(BaseModel):
    company_values: str
    summary: str
    logic: str
    concreteness: str
    fit: str
    delivery: str
    next_tips: List[str]
    example_answer: str
    retrieved_sources: List[str]



class SearchDocumentsRequest(BaseModel):
    job_family: str
    question: str
    answer: str
    top_k: int = 3

class SearchDocumentsResponse(BaseModel):
    top_question_docs: list
    top_answer_docs: list
    competency_docs: list
    answer_pattern_docs: list

# êµ¬ì¡°í™”ëœ í”¼ë“œë°± ìš”ì²­/ì‘ë‹µ (ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ ë°˜í™˜)
class StructuredFeedbackRequest(BaseModel):
    job_family: str
    question: str
    answer: str
    top_k: int = 3
# 2. í”„ë¡¬í”„íŠ¸ ì •ì˜


SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¸ì‚¬ë‹´ë‹¹ìì´ì ë©´ì ‘ ì½”ì¹˜ì…ë‹ˆë‹¤.

ì§€ì›ìì˜ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
- ë…¼ë¦¬ì„±: ë§ì˜ íë¦„, êµ¬ì¡°, í•µì‹¬ ë©”ì‹œì§€ê°€ ëª…í™•í•œì§€
- êµ¬ì²´ì„±: ê²½í—˜, ìˆ˜ì¹˜, ê¸°ê°„, ì—­í•  ë“± êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆëŠ”ì§€
- ì§ë¬´ ì í•©ì„±: ë‹µë³€ì´ ì§€ì› ì§ë¬´ì™€ ì˜ ì—°ê²°ë˜ëŠ”ì§€
- ì „ë‹¬ë ¥: ë©´ì ‘ ìƒí™©(1~2ë¶„ ë‹µë³€)ì— ë§ê²Œ í•µì‹¬ì´ ì˜ ì „ë‹¬ë˜ëŠ”ì§€

ì§€ì›ìê°€ ë°”ë¡œ ë‹¤ìŒ ì—°ìŠµì— í™œìš©í•  ìˆ˜ ìˆë„ë¡
'ë¬´ì—‡ì„ ì–´ë–»ê²Œ ê³ ì³ì•¼ í•˜ëŠ”ì§€'ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.
ë§‰ì—°í•œ ì¹­ì°¬ë³´ë‹¤ í–‰ë™ ì§€ì¹¨ì— ê°€ê¹Œìš´ í”¼ë“œë°±ì„ ì„ í˜¸í•©ë‹ˆë‹¤.
"""

def build_user_prompt(
    job: str,
    company: str,
    question: str,
    answer: str,
    context_docs: str = ""
) -> str:
    job_desc = job or "íŠ¹ì • ì§ë¬´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë©´ì ‘ ì—­ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”."
    company_desc = company or "íŠ¹ì • íšŒì‚¬ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    context_part = context_docs if context_docs else "ë³„ë„ì˜ ì°¸ê³  ìë£ŒëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    return dedent(f"""
    [ì§ë¬´ ì •ë³´]
    {job_desc}

    [íšŒì‚¬ ì •ë³´]
    {company_desc}

    [ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (ì°¸ê³  ìë£Œ)]
    {context_part}

    [ë©´ì ‘ ì§ˆë¬¸]
    {question}

    [ì§€ì›ìì˜ ë‹µë³€]
    {answer}

    [ìš”ì²­ì‚¬í•­]
    1. ìœ„ ë‹µë³€ì— ëŒ€í•´ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.
       - ì–´ë–¤ ì ì´ ì¢‹ê³  ì–´ë–¤ ì ì´ ë¶€ì¡±í•œì§€ ì„œìˆ 
       - í•„ìš”í•œ ê²½ìš° ê°œì„  ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”

    2. í•´ë‹¹ ë‹µë³€ì„ STAR êµ¬ì¡°ë¡œ ë¶„ì„í•˜ì—¬ ê° ìš”ì†Œë³„ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì½”ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
       - ì˜ˆì‹œ:
         - Situation: 3/5 - ë°°ê²½ ì„¤ëª…ì´ ëª¨í˜¸í•¨
         - Task: 4/5 - ëª©í‘œê°€ ëª…í™•í•˜ê²Œ í‘œí˜„ë¨
         - Action: 2/5 - êµ¬ì²´ì ì¸ í–‰ë™ ì„¤ëª…ì´ ë¶€ì¡±í•¨
         - Result: 3/5 - ì„±ê³¼ê°€ ëšœë ·í•˜ì§€ ì•ŠìŒ

    3. ê°œì„ ëœ ë‹µë³€(ëª¨ë²”ë‹µë³€)ì„ STAR êµ¬ì¡°ì— ë§ì¶° ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
       - ê° ë‹¨ê³„ë³„ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    ```json
    {{
      "summary": "ì „ë°˜ í‰ê°€ 2~3ë¬¸ì¥",
      "company_values": "ê¸°ì—… ì¸ì¬ìƒ ìš”ì•½ 2~3ë¬¸ì¥ + í˜„ì¬ ì¸ì¬ìƒì´ ìš”êµ¬í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë©´ì ‘ìê°€ ì–´ë–¤ ì‹ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•˜ë©´ ì¢‹ì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ëŠ” ì½”ë©˜íŠ¸ (3~5ë¬¸ì¥)",
      "logic": "ë…¼ë¦¬ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "concreteness": "êµ¬ì²´ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "fit": "ì§ë¬´ ì í•©ì„±ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "delivery": "ì „ë‹¬ë ¥ì— ëŒ€í•œ í”¼ë“œë°± 3~5ë¬¸ì¥",
      "next_tips": [
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  1",
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  2",
        "ë‹¤ìŒ ì—°ìŠµ ì‹œ ì‹ ê²½ ì“¸ ì  3"
      ],
      "example_answer": "ê°œì„ ëœ ì˜ˆì‹œ ë‹µë³€(8~12ë¬¸ì¥)",
      "retrieved_sources": []
    }}
    ```

    ì„¤ëª… ë¬¸ì¥ì€ ì“°ì§€ ë§ê³ , ìœ„ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    """)



# 3. GPT í˜¸ì¶œ í•¨ìˆ˜


def call_gpt(job: str, company: str, question: str, answer: str, context_docs: str = "") -> str:
    user_prompt = build_user_prompt(job, company, question, answer, context_docs)

    resp = client.chat.completions.create(
        model="gpt-4.1-mini",  # í•„ìš”ì— ë”°ë¼ ëª¨ë¸ ë³€ê²½ ê°€ëŠ¥
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.4,
    )

    content = resp.choices[0].message.content
    return content



# 4. JSON íŒŒì„œ


def parse_feedback(raw: str) -> FeedbackResponse:
    cleaned = raw.strip()

    # ```json ... ``` ê°ì‹¸ì§„ ê²½ìš° ì œê±°
    if cleaned.startswith("```"):
        cleaned = cleaned.strip("`")
        if cleaned.startswith("json"):
            cleaned = cleaned[4:]
        cleaned = cleaned.strip()

    try:
        data = json.loads(cleaned)
    except Exception:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ: raw ì „ì²´ë¥¼ summaryì— ë„£ê³  ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ê°’
        return FeedbackResponse(
            company_values=cleaned[:500],
            summary=cleaned[:500],
            logic="",
            concreteness="",
            fit="",
            delivery="",
            next_tips=[],
            example_answer="",
            retrieved_sources=[],
        )
    company_values = data.get("company_values", "")
    summary = data.get("summary", "")
    logic = data.get("logic", "")
    concreteness = data.get("concreteness", "")
    fit = data.get("fit", "")
    delivery = data.get("delivery", "")
    next_tips = data.get("next_tips", [])
    example_answer = data.get("example_answer", "")
    retrieved_sources = data.get("retrieved_sources", [])

    # íƒ€ì… ë°©ì–´: next_tips / retrieved_sources ê°€ ë¬¸ìì—´ë¡œ ì˜¬ ìˆ˜ë„ ìˆìŒ
    if isinstance(next_tips, str):
        next_tips = [next_tips]
    if not isinstance(next_tips, list):
        next_tips = []

    if isinstance(retrieved_sources, str):
        retrieved_sources = [retrieved_sources]
    if not isinstance(retrieved_sources, list):
        retrieved_sources = []

    return FeedbackResponse(
        company_values=company_values,
        summary=summary,
        logic=logic,
        concreteness=concreteness,
        fit=fit,
        delivery=delivery,
        next_tips=next_tips,
        example_answer=example_answer,
        retrieved_sources=retrieved_sources,
    )

# 5. ê¸°ì—… ì¸ì¬ìƒ
def load_company_values(path="./embedding/company_values.json"):
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

company_values_data = load_company_values()

def get_company_values_text(company: str) -> str:
    for item in company_values_data:
        meta = item.get("metadata", {})
        
        # company_name ë˜ëŠ” company_id ì¤‘ í•˜ë‚˜ ë§¤ì¹­ë˜ë©´ ì‚¬ìš©
        if meta.get("company_name") == company or meta.get("company_id") == company:
            return item["page_content"]

    return ""

# 6. FastAPI ì—”ë“œí¬ì¸íŠ¸ /ë‹µë³€ í…ŒìŠ¤íŠ¸ìš© fast api



app = FastAPI()



# ê¸°ì¡´ í”¼ë“œë°± ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜)

@app.post("/ai-feedback", response_model=FeedbackResponse)
async def ai_feedback(request: FeedbackRequest):
    company_values_text = get_company_values_text(request.company)
    context_docs = ""
    sources = []

    # í‘œì¤€ ì§ˆë¬¸ì¼ ê²½ìš° question_idë¡œ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ë„íë¨¼íŠ¸ ê²€ìƒ‰
    if request.standard_question == "y" and request.question_id:
        # question_templates featureì—ì„œ question_id ì¼ì¹˜í•˜ëŠ” ë„íë¨¼íŠ¸ ì¶”ì¶œ
        all_docs = vectorstore.docstore._dict.values()
        doc = next((d for d in all_docs if d.metadata.get("feature") == "question_templates" and d.metadata.get("question_id") == request.question_id), None)
        if doc:
            context_docs = doc.page_content
            sources = [request.question_id]
        else:
            print(f"question_id {request.question_id}ì— í•´ë‹¹í•˜ëŠ” ë„íë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # í‘œì¤€ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ê¸°ì¡´ RAG ê²€ìƒ‰
    if not context_docs:
        try:
            context_docs, sources = await get_rag_context(
                request.job_group,
                request.job,
                request.company,
                request.question,
                request.answer,
            )
        except Exception as e:
            print(" RAG ì—ëŸ¬, ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰:", e)
            context_docs = ""
            sources = []

    if company_values_text:
        context_docs = company_values_text + "\n\n---\n\n" + context_docs
        sources.insert(0, f"{request.company}-company-values")

    raw = call_gpt(
        job=request.job,
        company=request.company,
        question=request.question,
        answer=request.answer,
        context_docs=context_docs,
    )
    feedback = parse_feedback(raw)
    feedback.retrieved_sources = sources
    return feedback


# ìƒˆë¡œìš´ ì§ˆë¬¸ ëœë¤ ì¶”ì¶œ ì—”ë“œí¬ì¸íŠ¸
import random

@app.post("/random-questions", response_model=RandomQuestionResponse)
async def random_questions(request: RandomQuestionRequest):
    # 1. ë²¡í„°ìŠ¤í† ì–´ì—ì„œ question_templatesë§Œ ì¶”ì¶œ
    all_docs = vectorstore.docstore._dict.values()
    question_docs = [doc for doc in all_docs if doc.metadata.get("feature") == "question_templates"]

    # 2. q_listì— í•´ë‹¹í•˜ëŠ” question_typeë³„ë¡œ ê·¸ë£¹í•‘
    groups = []
    for q_type in request.q_list:
        filtered = [doc for doc in question_docs if doc.metadata.get("question_type") == q_type]
        # 3. ëœë¤ ì¶”ì¶œ (ì¤‘ë³µ ì—†ì´, ê°œìˆ˜ ë¶€ì¡±í•˜ë©´ ëª¨ë‘ ë°˜í™˜)
        selected = random.sample(filtered, min(request.count, len(filtered))) if filtered else []
        # 4. dictë¡œ ë³€í™˜ (FastAPI ì‘ë‹µ ì§ë ¬í™”)
        questions = [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in selected]
        groups.append(RandomQuestionGroup(question_type=q_type, questions=questions))

    return RandomQuestionResponse(groups=groups)



@app.post("/search-documents", response_model=SearchDocumentsResponse)
async def search_documents(request: SearchDocumentsRequest = Body(...)):
    # 1. job_familyë¡œ 1ì°¨ í•„í„°ë§
    all_docs = list(vectorstore.docstore._dict.values())
    filtered_docs = [doc for doc in all_docs if doc.metadata.get('job_family') == request.job_family]
    

    # 2. ì§ˆë¬¸/ë‹µë³€ featureë³„ í›„ë³´êµ° ìƒì„±
    question_docs = [doc for doc in filtered_docs if doc.metadata.get('feature') == 'question']
    answer_docs = [doc for doc in filtered_docs if doc.metadata.get('feature') == 'answer']

    # í›„ë³´êµ° ë‚´ì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ top_k ì¶”ì¶œ
    def similarity_search_in_candidates(query, candidates, k):
        if not candidates:
            return []
        # í›„ë³´êµ°ì˜ ì„ë² ë”© ì¶”ì¶œ
        texts = [doc.page_content for doc in candidates]
        query_emb = embedding.embed_query(query)
        doc_embs = embedding.embed_documents(texts)
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        def cosine_sim(a, b):
            a = np.array(a)
            b = np.array(b)
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        sims = [cosine_sim(query_emb, doc_emb) for doc_emb in doc_embs]
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ kê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ
        top_indices = np.argsort(sims)[::-1][:k]
        return [candidates[i] for i in top_indices]

    top_question_docs = similarity_search_in_candidates(request.question, question_docs, request.top_k)
    top_answer_docs = similarity_search_in_candidates(request.answer, answer_docs, request.top_k)

    # 3. answer_docsì˜ competency_tags ì·¨í•© ë° ê´€ë ¨ ë„íë¨¼íŠ¸ ì¶”ê°€
    competency_tags = set()
    for doc in top_answer_docs:
        tags = doc.metadata.get('competency_tags', [])
        if isinstance(tags, list):
            competency_tags.update(tags)
    competency_docs = [doc for doc in all_docs if doc.metadata.get('competency_id') in competency_tags]

    # 4. question_docsì˜ question_type ê¸°ë°˜ answer_patterns ë„íë¨¼íŠ¸ ì¶”ê°€
    question_types = set(doc.metadata.get('question_type') for doc in top_question_docs)
    answer_pattern_docs = [doc for doc in all_docs if doc.metadata.get('feature') == 'answer_patterns' and doc.metadata.get('question_type') in question_types]

    # FastAPI ì§ë ¬í™”: Document ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
    def doc_to_dict(doc):
        return {"page_content": doc.page_content, "metadata": doc.metadata}

    return SearchDocumentsResponse(
        top_question_docs=[doc_to_dict(doc) for doc in top_question_docs],
        top_answer_docs=[doc_to_dict(doc) for doc in top_answer_docs],
        competency_docs=[doc_to_dict(doc) for doc in competency_docs],
        answer_pattern_docs=[doc_to_dict(doc) for doc in answer_pattern_docs],
    )

@app.post("/structured-feedback")
async def structured_feedback(req: StructuredFeedbackRequest = Body(...)):
    # ë‚´ë¶€ì ìœ¼ë¡œ ê¸°ì¡´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
    search_res = await search_documents(SearchDocumentsRequest(
        job_family=req.job_family,
        question=req.question,
        answer=req.answer,
        top_k=req.top_k,
    ))

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    ctx_parts = []
    for d in search_res.top_question_docs:
        ctx_parts.append(f"[ë©´ì ‘ ì§ˆë¬¸ ì˜ˆì‹œ]\n{d['page_content']}")
    for d in search_res.top_answer_docs:
        ctx_parts.append(f"[ë‹µë³€ ì˜ˆì‹œ]\n{d['page_content']}")
    for d in search_res.competency_docs:
        name = d.get('metadata', {}).get('competency_name') or d.get('metadata', {}).get('competency_id')
        ctx_parts.append(f"[í•µì‹¬ ì—­ëŸ‰ ì„¤ëª… - {name}]\n{d['page_content']}")
    for d in search_res.answer_pattern_docs:
        pattern = d.get('metadata', {}).get('structure_name') or d.get('metadata', {}).get('question_type')
        ctx_parts.append(f"[ì¶”ì²œ ë‹µë³€ êµ¬ì¡° - {pattern}]\n{d['page_content']}")

    context_block = "\n\n".join(ctx_parts)

    # ìš”ì²­í•œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = dedent(f"""
    [ì°¸ê³ ìë£Œ]
    {context_block}

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {req.question}

    [ì‚¬ìš©ì ë‹µë³€]
    {req.answer}

    ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    ì¶œë ¥ ì‹œ ë°˜ë“œì‹œ Markdown í˜•ì‹ì„ ìœ ì§€í•˜ê³ ,
    ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„(\\n\\n)ì„ ë„£ìœ¼ì„¸ìš”.
    
    "ì¶œë ¥ ì‹œ ì ˆëŒ€ë¡œ Markdown ê·œì¹™ì„ ê¹¨ì§€ ë§ê³ , ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ì¤„(\\n\\n)ì„ ë„£ìœ¼ì„¸ìš”.\n"
    "ë¦¬ìŠ¤íŠ¸ í•­ëª©ì€ ê° í•­ëª© ë’¤ì— ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆì„ ìœ ì§€í•˜ì„¸ìš”.\n"
    "ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. í˜•ì‹ ë°– ì–´ë–¤ ë‚´ìš©ë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\n"


    ### ì „ì²´ ì´í‰

    ì „ì²´ ì´í‰ 5~6ì¤„

    ### ê°œì„ í¬ì¸íŠ¸
    ì‚¬ìš©ìì˜ ë‹µë³€ì—ì„œ ê°œì„ ì´ ì™œ í•„ìš”í•˜ê³ , ì–´ë–»ê²Œ ê°œì„ í•´ì•¼í• ì§€ì— ëŒ€í•´ ì„¤ëª…
    - [ê°œì„ í¬ì¸íŠ¸1]
    - [ê°œì„ í¬ì¸íŠ¸2]
    - [ê°œì„ í¬ì¸íŠ¸3]

    ### ëª¨ë²”ë‹µë³€ ì˜ˆì‹œ
    ì‚¬ìš©ì ì‹¤ì œ ë‹µë³€, ê·¸ì—ëŒ€í•œ ì „ì²´ ì´í‰ê³¼ ê°œì„ í¬ì¸íŠ¸ë¥¼ ë°˜ì˜í•œ ìƒˆë¡œìš´ ë‹µë³€ ìƒì„±
    ë‘ê´„ì‹ìœ¼ë¡œ ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ìœ¼ë¡œ êµ¬ì¡°í™”
    [ì„œë¡ ] : ì„œ-ë³¸-ê²°ì— ëŒ€í•œ í•µì‹¬ ë©”ì‹œì§€
    [ë³¸ë¡ ] : êµ¬ì²´ì ì¸ ê²½í—˜ í˜¹ì€ ì‚¬ë¡€ ì–¸ê¸‰
    [ê²°ë¡ ] : ë°°ìš´ì  í˜¹ì€ í–¥í›„ ë°œì „ê³„íš
    """)

    # ëª¨ë¸ í˜¸ì¶œ
    resp = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "í•œêµ­ì–´ ë©´ì ‘ ì½”ì¹˜ë¡œì„œ ê°„ê²°í•˜ê³  ì‹¤ì§ˆì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.4,
    )

    return {"markdown": resp.choices[0].message.content}